# Capstone Project Brainstorm â€” Winter 2026 DE Bootcamp
**Date:** 2026-02-20
**Context:** 5-week Data Engineering Bootcamp (Databricks-heavy) + Analytics Engineering Bootcamp

---

## Bootcamp Tool Stack

### DE Bootcamp
- Week 1: Data Lakes & Delta Tables
- Week 2: Databricks & Advanced Spark
- Week 3: Managing Unstructured Data
- Week 4: Structured Streaming â€” Kafka â†’ Delta Live Tables
- Week 5: Building AI Agents with Databricks

### AE Bootcamp (fair game for capstone)
- Airflow (orchestration, Iceberg loading, data quality, backfilling)
- Analytical patterns (growth accounting, change data capture)
- dbt (basics + advanced)
- Snowflake advanced (VARIANT type, Cortex AI: AI_PARSE_DOCUMENT, AI_EXTRACT, AI_EMBED)

---

## Project Requirements
- Must have a **pipeline**
- **Data quality controls**
- **Documentation and visualization**
- Must be **deployed in the cloud**
- **Conceptual data model** due by end of Week 3
- *(Today: last day of Week 1)*

---

## Data Source Direction
Health-related data: CDC, WHO, and related public health APIs/datasets.

---

## ðŸ’¡ Idea 1: Public Health Intelligence Platform *(Recommended â€” covers the most ground)*

**Core concept**: A multi-layered pipeline that ingests both **structured** (epidemiological statistics) and **unstructured** (WHO/CDC PDF reports, bulletins) health data, transforms it for analytics, and surfaces insights via an AI agent.

### Data Sources
- [CDC PLACES](https://www.cdc.gov/places/) â€” county-level chronic disease prevalence (diabetes, obesity, heart disease, mental health)
- [CDC Wonder API](https://wonder.cdc.gov/) â€” mortality/morbidity by cause of death
- [WHO Global Health Observatory API](https://www.who.int/data/gho/info/gho-odata-api) â€” global health indicators
- CDC/WHO PDF reports and guidelines (unstructured)

### Pipeline Architecture

```
[CDC/WHO APIs + PDFs]
        â†“
[Airflow] â€” orchestrates batch ingestion
        â†“
[Delta Lake / Iceberg] â€” raw / bronze layer
        â†“
[Spark on Databricks] â€” cleaning, enrichment (silver layer)
        â†“
[Delta Live Tables] â€” quality-checked, curated gold layer
        â†“
[dbt on Snowflake] â€” analytical models (growth accounting, CDC patterns)
        â†“
[Cortex AI (AI_PARSE_DOCUMENT, AI_EXTRACT)] â€” extract insights from PDFs
        â†“
[Databricks AI Agent] â€” answer natural language health questions
        â†“
[Dashboard] â€” Databricks SQL Dashboard or similar
```

### Why It Works
- âœ… Week 3 unstructured data (PDFs â†’ Spark NLP / Cortex AI)
- âœ… Week 4 streaming (Kafka â†’ Delta Live Tables for real-time WHO alerts)
- âœ… Week 5 AI agents
- âœ… Airflow orchestration + Iceberg from AE bootcamp
- âœ… dbt models + Snowflake Cortex from AE bootcamp
- âœ… Growth accounting patterns (e.g., how chronic disease rates change YoY across counties)
- âœ… Data quality via Delta Live Table expectations + dbt tests

---

## ðŸ’¡ Idea 2: Chronic Disease & Social Determinants Correlator *(Focused, analytically rich)*

**Core concept**: Correlate CDC chronic disease data (PLACES dataset) with social determinants of health (income, education, food access) to identify high-risk communities. Heavy analytics engineering focus.

### What Makes It Interesting
- CDC PLACES has 36 health measures across 27,000+ US census tracts â€” great for spatial analytics
- Join with Census Bureau data (ACS) for socioeconomic variables
- Use **change data capture** patterns (from AE bootcamp) to track how county-level metrics shift over annual releases
- **Growth accounting** to measure which conditions are worsening fastest in which regions
- dbt for all transformation logic
- Snowflake Cortex `AI_EMBED` to create semantic search over health measure descriptions

### Streaming Angle
Simulate a streaming layer with Kafka publishing new "data release" events triggering pipeline refreshes via Delta Live Tables.

---

## ðŸ’¡ Idea 3: Vaccine Coverage vs. Disease Outbreak Tracker *(Streaming-forward)*

**Core concept**: Real-time pipeline tracking vaccine coverage rates against disease incidence, with anomaly detection.

### Data Sources
- CDC vaccine coverage data (NIS â€” National Immunization Survey)
- CDC disease incidence data (Wonder API)
- WHO immunization data (GHO API)

### Streaming Angle
- Kafka topics simulating incoming health surveillance reports (state-level weekly disease counts)
- Delta Live Tables consuming the stream, applying quality expectations
- Databricks AI agent detects statistical anomalies (e.g., measles uptick in low-vaccination counties)

**Best fit if**: You want the streaming/Kafka components to be front-and-center.

---

## ðŸ’¡ Idea 4: Health Document RAG Pipeline *(AI/unstructured-forward)*

**Core concept**: Ingest CDC/WHO clinical guidelines, policy documents, and research summaries â†’ build a retrieval-augmented generation (RAG) system for querying public health knowledge.

- Spark handles large-scale PDF extraction (Week 3)
- Snowflake Cortex `AI_PARSE_DOCUMENT` + `AI_EMBED` for embeddings
- Databricks AI agent (Week 5) answers questions like *"What does the CDC recommend for pre-diabetes screening in adults over 45?"*
- Delta Lake stores document metadata + embedding vectors
- Airflow orchestrates nightly ingestion of new CDC/WHO publications

**Best fit if**: You're interested in the AI/LLM engineering side of things.

---

## ðŸ§­ Recommendation

**Idea 1** gives the best coverage of tools from both bootcamps and would make the strongest portfolio piece. **Idea 2** may be more tractable timeline-wise for a cleaner scope.

### Open Questions to Narrow Down
1. **Streaming vs. AI/unstructured emphasis?** (Week 4 vs. Week 5 focus)
2. **Existing cloud accounts?** (Snowflake + Databricks workspace already set up?)
3. **US-only or global datasets?**
4. **Specific health topic preference?** (chronic disease, infectious disease, mental health, maternal health, etc.)
